{
  "ai_position_title": "AI 專案程式碼生成器",
  "project_files": [
    {
      "file_path": "api/index.py",
      "file_content": "from flask import Flask, jsonify, render_template\nimport datetime\nimport os\nfrom .scraper.core import setup_google_sheets_client, collect_and_prepare_data, append_data_to_sheet\n\n# Vercel 部署的關鍵：\n# 由於我們的 Flask 應用 (index.py) 在 'api/' 子目錄中，\n# 我們需要明確告訴 Flask 去父目錄中尋找 'templates' 和 'static' 資料夾。\napp = Flask(__name__, template_folder='../templates', static_folder='../static')\n\n@app.route('/')\ndef home_page():\n    \"\"\"顯示一個簡單的狀態頁面，確認服務正在運行。\"\"\"\n    return render_template('index.html')\n\n@app.route('/api/scrape')\ndef trigger_scrape_workflow():\n    \"\"\"\n    這是由 Vercel Cron Job 觸發的核心端點。\n    它執行完整的抓取、處理和儲存流程。\n    \"\"\"\n    # 準備一個統一的時間戳\n    process_timestamp = datetime.datetime.utcnow().isoformat() + 'Z'\n\n    try:\n        # 步驟 1: 設定 Google Sheets 客戶端\n        gspread_client = setup_google_sheets_client()\n\n        # 步驟 2: 從目標網站抓取所有機場的等待時間資料\n        all_airport_data = collect_and_prepare_data()\n\n        if not all_airport_data:\n            raise RuntimeError(\"No data could be scraped from any of the target airports.\")\n\n        # 步驟 3: 將抓取到的資料附加到 Google Sheet\n        # 我們假設目標電子表格叫做 'TSA Wait Times'，工作表叫做 'RawData'\n        spreadsheet_name = \"TSA Wait Times\"\n        worksheet_name = \"RawData\"\n        append_data_to_sheet(gspread_client, spreadsheet_name, worksheet_name, all_airport_data)\n\n        # 如果一切順利，返回成功的 JSON 回應\n        success_response = {\n            \"status\": \"success\",\n            \"message\": \"Scraping job completed successfully.\",\n            \"data\": {\n                \"airports_processed\": list(set([row['Airport'] for row in all_airport_data])),\n                \"records_added\": len(all_airport_data)\n            },\n            \"timestamp\": process_timestamp\n        }\n        return jsonify(success_response)\n\n    except Exception as e:\n        # 如果過程中任何步驟出錯，捕獲異常並返回錯誤的 JSON 回應\n        # 這對於在 Vercel 日誌中進行調試非常重要\n        error_response = {\n            \"status\": \"error\",\n            \"message\": \"An error occurred during the scraping process.\",\n            \"error_details\": str(e),\n            \"timestamp\": process_timestamp\n        }\n        # 使用 HTTP 500 狀態碼表示伺服器錯誤\n        return jsonify(error_response), 500\n",
      "language": "python"
    },
    {
      "file_path": "api/scraper/__init__.py",
      "file_content": "",
      "language": "python"
    },
    {
      "file_path": "api/scraper/core.py",
      "file_content": "import os\nimport json\nimport requests\nimport gspread\nimport datetime\nfrom bs4 import BeautifulSoup\n\n# 定義我們要抓取的目標機場及其對應的網址\nTARGET_AIRPORTS = {\n    'JFK': 'https://www.jfkairport.com/at-the-airport/security-wait-times',\n    'LGA': 'https://www.laguardiaairport.com/at-the-airport/security-wait-times',\n    'EWR': 'https://www.ewr.com/at-the-airport/security-wait-times'\n}\n\ndef setup_google_sheets_client():\n    \"\"\"從環境變數讀取憑證並設定 gspread 客戶端。\"\"\"\n    # 從 Vercel 環境變數獲取服務帳戶的 JSON 憑證字串\n    gcp_creds_str = os.getenv('FIREBASE_SERVICE_ACCOUNT_JSON')\n    if not gcp_creds_str:\n        # 這是關鍵的錯誤處理，如果沒有設定憑證，我們就無法繼續\n        raise ValueError(\"[FIREBASE_SERVICE_ACCOUNT_JSON] 環境變數未設定。\")\n\n    # gspread 需要一個字典，所以我們解析 JSON 字串\n    gcp_creds_dict = json.loads(gcp_creds_str)\n\n    # 使用解析後的字典進行身份驗證\n    client = gspread.service_account_from_dict(gcp_creds_dict)\n    return client\n\ndef scrape_single_airport(airport_code, url):\n    \"\"\"抓取單一機場的安檢等待時間。\"\"\"\n    scraped_rows = []\n    try:\n        # 發送 HTTP GET 請求並設定超時以避免卡住\n        response = requests.get(url, timeout=15)\n        response.raise_for_status() # 如果請求失敗 (例如 404, 500), 就會拋出異常\n\n        # 使用 BeautifulSoup 解析 HTML 內容\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # 找到所有航廈的區塊\n        terminal_blocks = soup.find_all('div', class_='wait-times-terminal')\n        \n        # 準備一個統一的時間戳\n        scrape_timestamp = datetime.datetime.utcnow().isoformat() + 'Z'\n\n        # 遍歷每個航廈區塊\n        for terminal_block in terminal_blocks:\n            terminal_name_element = terminal_block.find('h2', class_='wait-times-terminal-name')\n            terminal_name = terminal_name_element.text.strip() if terminal_name_element else 'Unknown Terminal'\n\n            # 在每個航廈内找到所有的安檢點\n            checkpoint_blocks = terminal_block.find_all('div', class_='wait-times-checkpoint')\n            for checkpoint_block in checkpoint_blocks:\n                checkpoint_name_element = checkpoint_block.find('p', class_='wait-times-checkpoint-name')\n                wait_time_element = checkpoint_block.find('p', class_='wait-times-time')\n\n                if checkpoint_name_element and wait_time_element:\n                    checkpoint_name = checkpoint_name_element.text.strip()\n                    wait_time = wait_time_element.text.strip()\n                    \n                    # 組成一筆我們要儲存的紀錄\n                    record = {\n                        'Airport': airport_code,\n                        'Terminal': terminal_name,\n                        'Checkpoint': checkpoint_name,\n                        'WaitTime': wait_time,\n                        'ScrapeTimestamp': scrape_timestamp\n                    }\n                    scraped_rows.append(record)\n\n    except requests.RequestException as e:\n        # 如果網路請求失敗，印出錯誤訊息但繼續執行，避免整個任務失敗\n        print(f\"Could not fetch data for {airport_code}: {e}\")\n\n    return scraped_rows\n\ndef collect_and_prepare_data():\n    \"\"\"協調所有機場的資料抓取工作。\"\"\"\n    all_data = []\n    # 遍歷我們定義好的目標機場列表\n    for code, url in TARGET_AIRPORTS.items():\n        print(f\"Scraping data for {code}...\")\n        airport_data = scrape_single_airport(code, url)\n        if airport_data:\n            all_data.extend(airport_data)\n    return all_data\n\ndef append_data_to_sheet(client, spreadsheet_name, worksheet_name, data_rows):\n    \"\"\"將資料附加到指定的 Google Sheet 工作表中。\"\"\"\n    try:\n        # 開啟指定的電子表格和工作表\n        spreadsheet = client.open(spreadsheet_name)\n        worksheet = spreadsheet.worksheet(worksheet_name)\n    except gspread.exceptions.SpreadsheetNotFound:\n        raise RuntimeError(f\"Spreadsheet '{spreadsheet_name}' not found. Please create it and share it with the service account.\")\n    except gspread.exceptions.WorksheetNotFound:\n        raise RuntimeError(f\"Worksheet '{worksheet_name}' not found in '{spreadsheet_name}'. Please create it.\")\n\n    # 取得工作表的所有現有內容，檢查是否需要寫入表頭\n    existing_data = worksheet.get_all_records()\n    if not existing_data:\n        # 如果工作表是空的，先寫入表頭\n        header = ['Airport', 'Terminal', 'Checkpoint', 'WaitTime', 'ScrapeTimestamp']\n        worksheet.append_row(header)\n\n    # 將每一筆紀錄轉換成列表格式，並附加到工作表的末尾\n    rows_to_append = []\n    for row_dict in data_rows:\n        row_list = [row_dict['Airport'], row_dict['Terminal'], row_dict['Checkpoint'], row_dict['WaitTime'], row_dict['ScrapeTimestamp']]\n        rows_to_append.append(row_list)\n    \n    if rows_to_append:\n        worksheet.append_rows(rows_to_append)\n        print(f\"Successfully appended {len(rows_to_append)} rows to the sheet.\")\n",
      "language": "python"
    },
    {
      "file_path": "templates/index.html",
      "file_content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>TSA Wait Times Scraper Status</title>\n    <style>\n        body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif; display: flex; justify-content: center; align-items: center; height: 100vh; margin: 0; background-color: #f7f7f7; color: #333; text-align: center; }\n        .container { padding: 2rem; }\n        h1 { font-size: 1.5rem; color: #2c3e50; }\n        p { color: #7f8c8d; }\n        code { background-color: #ecf0f1; padding: 0.2rem 0.4rem; border-radius: 4px; color: #34495e; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>TSA Wait Times Scraper Service</h1>\n        <p>Service is running. The scraping process is triggered automatically via <code>/api/scrape</code> endpoint.</p>\n    </div>\n</body>\n</html>",
      "language": "html"
    },
    {
      "file_path": "static/css/style.css",
      "file_content": "/* This file is intentionally left blank. */\n/* All styles for the status page are included inline in templates/index.html for simplicity and to minimize requests. */",
      "language": "css"
    },
    {
      "file_path": "requirements.txt",
      "file_content": "Flask\nrequests\nbeautifulsoup4\ngspread\ngoogle-auth-oauthlib\ngoogle-api-python-client",
      "language": "plaintext"
    },
    {
      "file_path": "vercel.json",
      "file_content": "{\n  \"builds\": [\n    {\n      \"src\": \"api/index.py\",\n      \"use\": \"@vercel/python\"\n    }\n  ],\n  \"routes\": [\n    { \"src\": \"/(.*)\", \"dest\": \"api/index.py\" }\n  ],\n  \"crons\": [\n    {\n      \"path\": \"/api/scrape\",\n      \"schedule\": \"0 * * * *\"\n    }\n  ]\n}",
      "language": "json"
    }
  ]
}